apiVersion: batch/v1
kind: Job
metadata:
  name: qwen2-5-0-5b
  namespace: rhaiis-namespace
spec:
  backoffLimit: 3
  template:
    metadata:
      labels:
        app: qwen2-5-0-5b
    spec:
      restartPolicy: OnFailure
      imagePullSecrets:
        - name: docker-secret
      volumes:
        - name: model-volume
          persistentVolumeClaim:
            claimName: qwen2-5-0-5b
      containers:
        - name: download-model
          image: 'registry.redhat.io/rhaiis/vllm-cuda-rhel9@sha256:a6645a8e8d7928dce59542c362caf11eca94bb1b427390e78f0f8a87912041cd'
          imagePullPolicy: IfNotPresent
          envFrom:
            - secretRef:
                name: hf-secret
          env:
            - name: HF_HOME
              value: /opt/app-root/src/.cache
            - name: HF_HUB_OFFLINE
              value: "0"
            - name: MODEL_NAME
              value: "Qwen/Qwen2.5-0.5B"
          command:
            - /bin/bash
            - -c
            - |
              echo "Starting model download from HuggingFace..."
              echo "Model: $MODEL_NAME"
              echo "Download location: $HF_HOME"

              # Install huggingface-cli if not available
              pip install -q huggingface_hub

              # Download the model
              huggingface-cli download "$MODEL_NAME" \
                --local-dir "$HF_HOME/hub/models--${MODEL_NAME//\//_}" \
                --local-dir-use-symlinks False

              echo "Model download completed successfully!"
              ls -lh "$HF_HOME/hub/"
          volumeMounts:
            - name: model-volume
              mountPath: /opt/app-root/src/.cache
          resources:
            limits:
              cpu: '4'
              memory: 8Gi
              nvidia.com/gpu: '1'
            requests:
              cpu: '1'
              memory: 2Gi
              nvidia.com/gpu: '1'
      tolerations:
        - effect: NoSchedule
          key: g6e-gpu
          operator: Exists
