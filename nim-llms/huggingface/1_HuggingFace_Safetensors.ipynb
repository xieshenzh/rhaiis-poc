{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51206717",
   "metadata": {},
   "source": [
    "# Deploy HuggingFace with RHAIIS on OpenShift\n",
    "\n",
    "## What's Covered\n",
    "\n",
    "This tutorial includes:\n",
    "* **Example 1**: Basic deployment from HuggingFace (5 minutes)\n",
    "* **Example 2**: Customizing model parameters\n",
    "* **Example 3**: Deploying from local storage for offline use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ritjty1clk",
   "source": [
    "## Setup: Python SDK with uv for IntelliJ\n",
    "\n",
    "### Prerequisites Setup\n",
    "\n",
    "This notebook uses `uv` for Python dependency management. Follow these steps to set up your environment:\n",
    "\n",
    "#### 1. Initialize uv project\n",
    "```bash\n",
    "uv init --no-readme\n",
    "```\n",
    "\n",
    "#### 2. Add Jupyter dependencies to pyproject.toml\n",
    "```bash\n",
    "uv add jupyter notebook ipykernel ipywidgets\n",
    "```\n",
    "\n",
    "Or manually edit `pyproject.toml`:\n",
    "```toml\n",
    "dependencies = [\n",
    "    \"jupyter>=1.0.0\",\n",
    "    \"notebook>=7.0.0\",\n",
    "    \"ipykernel>=6.0.0\",\n",
    "    \"ipywidgets>=8.0.0\",\n",
    "]\n",
    "```\n",
    "\n",
    "#### 3. Sync/install dependencies\n",
    "```bash\n",
    "uv sync\n",
    "```\n",
    "\n",
    "This creates a `.venv` virtual environment and installs all packages.\n",
    "\n",
    "#### 4. Configure IntelliJ IDEA\n",
    "1. Open **File → Project Structure → Project**\n",
    "2. Click **SDK** → **Add SDK** → **Python SDK**\n",
    "3. Select **Virtualenv Environment** → **Existing environment**\n",
    "4. Browse to: `rhaiis-poc/.venv/bin/python`\n",
    "5. Click **OK**\n",
    "\n",
    "#### 5. Use the SDK in this notebook\n",
    "1. In IntelliJ, select the Python interpreter (the one you just configured)\n",
    "2. The notebook will use the Jupyter kernel from your `.venv`\n",
    "\n",
    "---\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "0707e495",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "\n",
    "Below are some utility functions we'll use in this notebook. These are for simplifying the process of deploying and monitoring NIMs in a notebook environment, and aren't required in general.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "bf4db25d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T19:24:09.367061Z",
     "start_time": "2026-02-04T19:24:09.338250Z"
    }
   },
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def check_service_ready(url):\n",
    "    \"\"\"Fallback health check using HTTP endpoint\"\"\"\n",
    "    url = f\"http://{url}/health\"\n",
    "    print(\"Checking service health endpoint...\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(url, headers={'accept': 'application/json'})\n",
    "            if response.status_code == 200 :\n",
    "                print(\"✓ Service ready!\")\n",
    "                break\n",
    "        except requests.ConnectionError:\n",
    "            pass\n",
    "        print(\"⏳ Still starting...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "def generate_text(url, model, prompt, max_tokens=1000, temperature=0.7):\n",
    "    \"\"\"Generate text using the NIM service\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"http://{url}/v1/chat/completions\",\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": temperature\n",
    "            },\n",
    "            timeout=60\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()['choices'][0]['message']['content']\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making request: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"✓ Utility functions loaded successfully\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Utility functions loaded successfully\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "1b3b24b1",
   "metadata": {},
   "source": [
    "## Deployment Examples\n",
    "\n",
    "Let's explore different ways to deploy models using NIM.\n",
    "\n",
    "### Example 1: Basic Deployment from Hugging Face\n",
    "\n",
    "This example shows how to deploy Llama-3.1-8B-Instruct with default settings directly from Hugging Face.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. Create the Secret custom resource (CR) for the Hugging Face token. The cluster uses the Secret CR to pull models from Hugging Face.",
   "id": "35e4a9502e80ed57"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1.1 Set the HF_TOKEN variable using the token you set in Hugging Face.",
   "id": "350b47c010921ad9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!HF_TOKEN=<your_huggingface_token>",
   "id": "a12fb443cad241d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1.2 Set the cluster namespace to match where you deployed the Red Hat AI Inference Server image, for example:",
   "id": "b68d74fae62ee2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!NAMESPACE=rhaiis-namespace",
   "id": "d7a5f6d38bec147f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1.3 Create the Secret CR in the cluster:",
   "id": "bceadcf6d8403098"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!oc create secret generic hf-secret --from-literal=HF_TOKEN=$HF_TOKEN -n $NAMESPACE",
   "id": "3ebd21c9fad71443",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Create the Docker secret so that the cluster can download the Red Hat AI Inference Server image from the container registry. For example, to create a Secret CR that contains the contents of your local ~/.docker/config.json file, run the following command:",
   "id": "9739ce24292f5eb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!oc create secret generic docker-secret --from-file=.dockercfg=$HOME/.docker/config.json --type=kubernetes.io/dockercfg -n rhaiis-namespace",
   "id": "26b53b4796bc8c14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Create a PersistentVolumeClaim (PVC) custom resource (CR) and apply it in the cluster. The following example PVC CR uses a default IBM VPC Block persistence volume. You use the PVC as the location where you store the models that you download.",
   "id": "38a11d9c9c092847"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!oc apply -f ./1_basic/pvc.yaml -n rhaiis-namespace",
   "id": "72842001ec52782b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4. Create a Deployment custom resource (CR) that pulls the model from Hugging Face and deploys the Red Hat AI Inference Server container. Reference the following example Deployment CR, which uses AI Inference Server to serve a Granite model on a CUDA accelerator.",
   "id": "8c3ce075c74fc2fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!oc apply -f ./1_basic/deployment.yaml -n rhaiis-namespace",
   "id": "6408ad6e55709b0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 5. Create a Service CR for the model inference. For example:",
   "id": "1dd3c25e42f08944"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!oc apply -f ./1_basic/service.yaml -n rhaiis-namespace",
   "id": "5a3c129686e1ed5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 6. Create a Route CR to enable public access to the model. For example:",
   "id": "c43aa6fd9092edcb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!oc apply -f ./1_basic/route.yaml -n rhaiis-namespace",
   "id": "e032ed4fa16d1fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "392df9b0",
   "metadata": {},
   "source": "#### 7. Now let's test the deployed model:"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check if service is ready:",
   "id": "881f76653e31163d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T19:24:53.240601Z",
     "start_time": "2026-02-04T19:24:52.167075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "endpoint = !oc get route llama-3-1-8b-instruct -n rhaiis-namespace -o jsonpath='{.spec.host}'\n",
    "\n",
    "# URL is a list, access the first element\n",
    "print(endpoint[0])\n",
    "\n",
    "check_service_ready(url=endpoint[0])"
   ],
   "id": "ea9af6b0a59e6c37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using insecure TLS client config. Setting this option is not supported!\r\n",
      "\r\n",
      "Logged into \"https://api.ai-dev06.kni.syseng.devcluster.openshift.com:6443\" as \"xiezhang@redhat.com\" using the token provided.\r\n",
      "\r\n",
      "You have access to 90 projects, the list has been suppressed. You can list all projects with 'oc projects'\r\n",
      "\r\n",
      "Using project \"default\".\r\n",
      "llama-3-1-8b-instruct-xieshen-rhaiis.apps.ai-dev06.kni.syseng.devcluster.openshift.com\n",
      "Checking service health endpoint...\n",
      "✓ Service ready!\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test the Service:",
   "id": "c5c0f510955a47f4"
  },
  {
   "cell_type": "code",
   "id": "f4c9b082",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T19:25:05.882015Z",
     "start_time": "2026-02-04T19:24:56.140139Z"
    }
   },
   "source": [
    "result = generate_text(\n",
    "    url=endpoint[0],\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    prompt=\"Write a complete function that computes fibonacci numbers in Rust\"\n",
    ")\n",
    "print(result if result else \"Failed to generate text\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Fibonacci Function in Rust**\n",
      "================================\n",
      "\n",
      "Here is a simple function that computes Fibonacci numbers in Rust. This function uses recursion, which is a common approach for calculating Fibonacci numbers. However, please note that recursion can lead to a stack overflow for large inputs.\n",
      "\n",
      "```rust\n",
      "/// Calculate the nth Fibonacci number using recursion.\n",
      "fn fibonacci_recursive(n: u32) -> u32 {\n",
      "    match n {\n",
      "        0 => 0,\n",
      "        1 => 1,\n",
      "        _ => fibonacci_recursive(n - 1) + fibonacci_recursive(n - 2),\n",
      "    }\n",
      "}\n",
      "\n",
      "/// Calculate the nth Fibonacci number using iteration.\n",
      "fn fibonacci_iterative(n: u32) -> u32 {\n",
      "    if n <= 1 {\n",
      "        return n;\n",
      "    }\n",
      "\n",
      "    let mut a = 0;\n",
      "    let mut b = 1;\n",
      "    for _ in 2..=n {\n",
      "        let temp = a + b;\n",
      "        a = b;\n",
      "        b = temp;\n",
      "    }\n",
      "    b\n",
      "}\n",
      "\n",
      "fn main() {\n",
      "    let n = 10; // Change this to the desired Fibonacci number\n",
      "    println!(\"Fibonacci number at index {} (recursive): {}\", n, fibonacci_recursive(n));\n",
      "    println!(\"Fibonacci number at index {} (iterative): {}\", n, fibonacci_iterative(n));\n",
      "}\n",
      "```\n",
      "\n",
      "**Performance Considerations**\n",
      "-----------------------------\n",
      "\n",
      "While the recursive function is simple and easy to understand, it is not the most efficient way to calculate Fibonacci numbers, especially for large inputs. This is because each recursive call creates a new stack frame, leading to a high time complexity of O(2^n).\n",
      "\n",
      "The iterative function, on the other hand, has a time complexity of O(n) and is generally faster and more memory-efficient.\n",
      "\n",
      "**Example Use Cases**\n",
      "---------------------\n",
      "\n",
      "You can use the `fibonacci_recursive` or `fibonacci_iterative` function to calculate any Fibonacci number. Simply pass the desired index `n` as an argument.\n",
      "\n",
      "```rust\n",
      "let n = 10; // Calculate the 10th Fibonacci number\n",
      "println!(\"Fibonacci number at index {}: {}\", n, fibonacci_recursive(n));\n",
      "```\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "d2e3ab77",
   "metadata": {},
   "source": [
    "### Example 2: Deployment Using Different Backend Options\n",
    "\n",
    "NIM supports multiple backends for model deployment. Let's explore TensorRT-LLM and vLLM backends:\n",
    "\n",
    "#### TensorRT-LLM Backend\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6759f234",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Using TensorRT-LLM backend by specifying the NIM_MODEL_PROFILE parameter\n",
    "!docker run -it --rm \\\n",
    " --name=$CONTAINER_NAME \\\n",
    " --runtime=nvidia \\\n",
    " --gpus all \\\n",
    " --shm-size=16GB \\\n",
    " -e HF_TOKEN=$HF_TOKEN \\\n",
    " -e NIM_MODEL_NAME=\"hf://mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_SERVED_MODEL_NAME=\"mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_MODEL_PROFILE=\"tensorrt_llm\" \\\n",
    " -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    " -u $(id -u) \\\n",
    " -p 8000:8000 \\\n",
    " -d \\\n",
    " $NIM_IMAGE"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "73a9eafe",
   "metadata": {},
   "source": [
    "check_service_ready_from_logs(os.environ[\"CONTAINER_NAME\"], print_logs=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "226b3734",
   "metadata": {},
   "source": [
    "Test the TensorRT-LLM backend:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b9a2f8a4",
   "metadata": {},
   "source": [
    "result = generate_text(\n",
    "    model=\"mistralai/Codestral-22B-v0.1\",\n",
    "    prompt=\"Write a complete Python function that computes fibonacci numbers with memoization\"\n",
    ")\n",
    "print(\"TensorRT-LLM Backend Result:\")\n",
    "print(\"=\" * 50)\n",
    "print(result if result else \"Failed to generate text\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0c25ffbd",
   "metadata": {},
   "source": [
    "Before we move onto the next example, let's stop the LLM NIM service.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "dc65b39b",
   "metadata": {},
   "source": [
    "!docker stop $CONTAINER_NAME 2>/dev/null || echo \"Container already stopped\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4f8e5f0a",
   "metadata": {},
   "source": [
    "#### vLLM Backend\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "470ce511",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Using vLLM backend by specifying the NIM_MODEL_PROFILE parameter\n",
    "!docker run -it --rm \\\n",
    " --name=$CONTAINER_NAME \\\n",
    " --runtime=nvidia \\\n",
    " --gpus all \\\n",
    " --shm-size=16GB \\\n",
    " -e HF_TOKEN=$HF_TOKEN \\\n",
    " -e NIM_MODEL_NAME=\"hf://mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_SERVED_MODEL_NAME=\"mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_MODEL_PROFILE=\"vllm\" \\\n",
    " -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    " -u $(id -u) \\\n",
    " -p 8000:8000 \\\n",
    " -d \\\n",
    " $NIM_IMAGE"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "50ef066d",
   "metadata": {},
   "source": [
    "check_service_ready_from_logs(os.environ[\"CONTAINER_NAME\"], print_logs=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "793b9643",
   "metadata": {},
   "source": [
    "Test the vLLM backend:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f7dbba8f",
   "metadata": {},
   "source": [
    "result = generate_text(\n",
    "    model=\"mistralai/Codestral-22B-v0.1\",\n",
    "    prompt=\"Write a complete C++ function that computes fibonacci numbers efficiently\"\n",
    ")\n",
    "print(\"vLLM Backend Result:\")\n",
    "print(\"=\" * 50)\n",
    "print(result if result else \"Failed to generate text\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d024eba7",
   "metadata": {},
   "source": [
    "Before we move onto the next example, let's stop the LLM NIM service.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1b2d02f8",
   "metadata": {},
   "source": [
    "!docker stop $CONTAINER_NAME 2>/dev/null || echo \"Container already stopped\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d0fed5f0",
   "metadata": {},
   "source": [
    "### Example 3: Customizing Model Parameters\n",
    "\n",
    "This example demonstrates how custom parameters affect model behavior. We'll deploy with specific constraints and test them:\n",
    "\n",
    "**Key Parameters:**\n",
    "* `NIM_TENSOR_PARALLEL_SIZE=2`: Uses 2 GPUs in parallel for better performance\n",
    "* `NIM_MAX_INPUT_LENGTH=2048`: Limits input to 2048 tokens\n",
    "* `NIM_MAX_OUTPUT_LENGTH=512`: Limits output to 512 tokens\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> You must have at least 2 GPUs to run the following cell. If you don't have at least 2 GPUs, modify the <code>NIM_TENSOR_PARALLEL_SIZE</code> paramater in the cell below.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d572e72f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "!docker run -it --rm \\\n",
    " --name=$CONTAINER_NAME \\\n",
    " --runtime=nvidia \\\n",
    " --gpus all \\\n",
    " --shm-size=16GB \\\n",
    " -e HF_TOKEN=$HF_TOKEN \\\n",
    " -e NIM_MODEL_NAME=\"hf://mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_SERVED_MODEL_NAME=\"mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_TENSOR_PARALLEL_SIZE=2 \\\n",
    " -e NIM_MAX_INPUT_LENGTH=2048 \\\n",
    " -e NIM_MAX_OUTPUT_LENGTH=512 \\\n",
    " -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    " -u $(id -u) \\\n",
    " -p 8000:8000 \\\n",
    " -d \\\n",
    " $NIM_IMAGE"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e596290",
   "metadata": {},
   "source": [
    "# Use the log-based check (set print_logs=True to see detailed logs)\n",
    "check_service_ready_from_logs(os.environ[\"CONTAINER_NAME\"], print_logs=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "43c8335c",
   "metadata": {},
   "source": [
    "Test with custom parameters:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "bc931c2a",
   "metadata": {},
   "source": [
    "result = generate_text(model=\"mistralai/Codestral-22B-v0.1\",\n",
    "                       prompt=\"Write me a function that computes fibonacci in Javascript\")\n",
    "print(result if result else \"Failed to generate text\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9bd6f477",
   "metadata": {},
   "source": [
    "Before we move onto the next example, let's stop the LLM NIM service.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e4aa1e62",
   "metadata": {},
   "source": [
    "!docker stop $CONTAINER_NAME 2>/dev/null || echo \"Container already stopped\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6d0f0713",
   "metadata": {},
   "source": [
    "### Example 4: Deployment from Local Model\n",
    "\n",
    "This example shows how to deploy Qwen2.5-0.5B from the locally downloaded model:\n",
    "\n",
    "#### Download Model to Local Storage\n",
    "\n",
    "We'll download Qwen2.5-0.5B, a lightweight LLM, for use in Example 4.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> You can modify the `model_save_location` variable below to use a different directory for storing downloaded models.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d88c9916",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Set up local model directory\n",
    "model_save_location = os.path.join(base_work_dir, \"models\")\n",
    "local_model_name = \"Qwen2.5-0.5B-Instruct\"\n",
    "local_model_path = os.path.join(model_save_location, local_model_name)\n",
    "os.makedirs(local_model_path, exist_ok=True)\n",
    "\n",
    "os.environ[\"LOCAL_MODEL_DIR\"] = local_model_path"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0a3e7351",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "!huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct --local-dir \"$LOCAL_MODEL_DIR\" && echo \"✓ Model downloaded successfully\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c0142da7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Verify model files exist\n",
    "!ls -Rlh \"$LOCAL_MODEL_DIR\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f07d6f8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "!docker run -it --rm \\\n",
    " --name=$CONTAINER_NAME \\\n",
    " --runtime=nvidia \\\n",
    " --gpus '\"device=0\"' \\\n",
    " --shm-size=16GB \\\n",
    " -e NIM_MODEL_NAME=\"/opt/models/local_model\" \\\n",
    " -e NIM_SERVED_MODEL_NAME=\"Qwen/Qwen2.5-0.5B\" \\\n",
    " -v \"$LOCAL_MODEL_DIR:/opt/models/local_model\" \\\n",
    " -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    " -u $(id -u) \\\n",
    " -p 8000:8000 \\\n",
    " -d \\\n",
    " $NIM_IMAGE"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9394bcee",
   "metadata": {},
   "source": [
    "# Use the log-based check (set print_logs=True to see detailed logs)\n",
    "check_service_ready_from_logs(os.environ[\"CONTAINER_NAME\"], print_logs=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b3808a1b",
   "metadata": {},
   "source": [
    "Test the local model deployment:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8228a12a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "result = generate_text(model=\"Qwen/Qwen2.5-0.5B\",\n",
    "                       prompt=\"Tell me a story about a cat\")\n",
    "print(result if result else \"Failed to generate text\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d0bb5365",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Final cleanup\n",
    "!docker stop $CONTAINER_NAME 2>/dev/null || echo \"Container already stopped\"\n",
    "print(\"✓ All containers stopped successfully\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5b169f55",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
