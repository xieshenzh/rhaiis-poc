{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51206717",
   "metadata": {},
   "source": "<!--\nSPDX-FileCopyrightText: Copyright (c) 2024-2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\nSPDX-License-Identifier: Apache-2.0\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n> **Note:** This file has been modified from the original\n> [NVIDIA AI Blueprint: Bring Your LLM to NIM](https://github.com/NVIDIA-AI-Blueprints/bring-llms-to-nim).\n> Changes: Adapted for Red Hat AI Inference Server (RHAIIS) on OpenShift,\n> replacing NVIDIA NIM Docker deployment with OpenShift Kubernetes resources.\n\n# Deploy HuggingFace with RHAIIS on OpenShift\n\n## What's Covered\n\nThis tutorial includes:\n* **Example 1**: Basic deployment from HuggingFace (5 minutes)\n* **Example 2**: Customizing model parameters\n* **Example 3**: Deploying from local storage for offline use"
  },
  {
   "cell_type": "markdown",
   "id": "3ritjty1clk",
   "source": [
    "## Setup: Python SDK with uv for IntelliJ\n",
    "\n",
    "### Prerequisites Setup\n",
    "\n",
    "This notebook uses `uv` for Python dependency management. Follow these steps to set up your environment:\n",
    "\n",
    "#### 1. Initialize uv project\n",
    "```bash\n",
    "uv init --no-readme\n",
    "```\n",
    "\n",
    "#### 2. Add Jupyter dependencies to pyproject.toml\n",
    "```bash\n",
    "uv add jupyter notebook ipykernel ipywidgets\n",
    "```\n",
    "\n",
    "Or manually edit `pyproject.toml`:\n",
    "```toml\n",
    "dependencies = [\n",
    "    \"jupyter>=1.0.0\",\n",
    "    \"notebook>=7.0.0\",\n",
    "    \"ipykernel>=6.0.0\",\n",
    "    \"ipywidgets>=8.0.0\",\n",
    "]\n",
    "```\n",
    "\n",
    "#### 3. Sync/install dependencies\n",
    "```bash\n",
    "uv sync\n",
    "```\n",
    "\n",
    "This creates a `.venv` virtual environment and installs all packages.\n",
    "\n",
    "#### 4. Configure IntelliJ IDEA\n",
    "1. Open **File → Project Structure → Project**\n",
    "2. Click **SDK** → **Add SDK** → **Python SDK**\n",
    "3. Select **Virtualenv Environment** → **Existing environment**\n",
    "4. Browse to: `rhaiis-poc/.venv/bin/python`\n",
    "5. Click **OK**\n",
    "\n",
    "#### 5. Use the SDK in this notebook\n",
    "1. In IntelliJ, select the Python interpreter (the one you just configured)\n",
    "2. The notebook will use the Jupyter kernel from your `.venv`\n",
    "\n",
    "---\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "0707e495",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "\n",
    "Below are some utility functions we'll use in this notebook. These are for simplifying the process of deploying and monitoring NIMs in a notebook environment, and aren't required in general.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "bf4db25d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T19:24:09.367061Z",
     "start_time": "2026-02-04T19:24:09.338250Z"
    }
   },
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def check_service_ready(url):\n",
    "    \"\"\"Fallback health check using HTTP endpoint\"\"\"\n",
    "    url = f\"http://{url}/health\"\n",
    "    print(\"Checking service health endpoint...\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(url, headers={'accept': 'application/json'})\n",
    "            if response.status_code == 200 :\n",
    "                print(\"✓ Service ready!\")\n",
    "                break\n",
    "        except requests.ConnectionError:\n",
    "            pass\n",
    "        print(\"⏳ Still starting...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "def generate_text(url, model, prompt, max_tokens=1000, temperature=0.7):\n",
    "    \"\"\"Generate text using the NIM service\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"http://{url}/v1/chat/completions\",\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": temperature\n",
    "            },\n",
    "            timeout=60\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()['choices'][0]['message']['content']\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making request: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"✓ Utility functions loaded successfully\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Utility functions loaded successfully\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "1b3b24b1",
   "metadata": {},
   "source": [
    "## Deployment Examples\n",
    "\n",
    "Let's explore different ways to deploy models using NIM.\n",
    "\n",
    "### Example 1: Basic Deployment from Hugging Face\n",
    "\n",
    "This example shows how to deploy Llama-3.1-8B-Instruct with default settings directly from Hugging Face.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. Create the Secret custom resource (CR) for the Hugging Face token. The cluster uses the Secret CR to pull models from Hugging Face.",
   "id": "35e4a9502e80ed57"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1.1 Set the HF_TOKEN variable using the token you set in Hugging Face.",
   "id": "350b47c010921ad9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!HF_TOKEN=<your_huggingface_token>",
   "id": "a12fb443cad241d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1.2 Set the cluster namespace to match where you deployed the Red Hat AI Inference Server image, for example:",
   "id": "b68d74fae62ee2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!NAMESPACE=rhaiis-namespace",
   "id": "d7a5f6d38bec147f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1.3 Create the Secret CR in the cluster:",
   "id": "bceadcf6d8403098"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!oc create secret generic hf-secret --from-literal=HF_TOKEN=$HF_TOKEN -n $NAMESPACE",
   "id": "3ebd21c9fad71443",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Create the Docker secret so that the cluster can download the Red Hat AI Inference Server image from the container registry. For example, to create a Secret CR that contains the contents of your local ~/.docker/config.json file, run the following command:",
   "id": "9739ce24292f5eb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!oc create secret generic docker-secret --from-file=.dockercfg=$HOME/.docker/config.json --type=kubernetes.io/dockercfg -n rhaiis-namespace",
   "id": "26b53b4796bc8c14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Create a PersistentVolumeClaim (PVC) custom resource (CR) and apply it in the cluster. You use the PVC as the location where you store the models that you download.",
   "id": "38a11d9c9c092847"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!oc apply -f ./1_basic/pvc.yaml -n rhaiis-namespace",
   "id": "72842001ec52782b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4. Create a Deployment custom resource (CR) that pulls the model from Hugging Face and deploys the Red Hat AI Inference Server container.",
   "id": "8c3ce075c74fc2fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!oc apply -f ./1_basic/deployment.yaml -n rhaiis-namespace",
   "id": "6408ad6e55709b0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 5. Create a Service CR for the model inference. For example:",
   "id": "1dd3c25e42f08944"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!oc apply -f ./1_basic/service.yaml -n rhaiis-namespace",
   "id": "5a3c129686e1ed5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 6. Create a Route CR to enable public access to the model. For example:",
   "id": "c43aa6fd9092edcb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!oc apply -f ./1_basic/route.yaml -n rhaiis-namespace",
   "id": "e032ed4fa16d1fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "392df9b0",
   "metadata": {},
   "source": "#### 7. Now let's test the deployed model:"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check if service is ready:",
   "id": "881f76653e31163d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T19:24:53.240601Z",
     "start_time": "2026-02-04T19:24:52.167075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "endpoint = !oc get route llama-3-1-8b-instruct -n rhaiis-namespace -o jsonpath='{.spec.host}'\n",
    "\n",
    "# URL is a list, access the first element\n",
    "print(endpoint[0])\n",
    "\n",
    "check_service_ready(url=endpoint[0])"
   ],
   "id": "ea9af6b0a59e6c37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using insecure TLS client config. Setting this option is not supported!\r\n",
      "\r\n",
      "Logged into \"https://api.ai-dev06.kni.syseng.devcluster.openshift.com:6443\" as \"xiezhang@redhat.com\" using the token provided.\r\n",
      "\r\n",
      "You have access to 90 projects, the list has been suppressed. You can list all projects with 'oc projects'\r\n",
      "\r\n",
      "Using project \"default\".\r\n",
      "llama-3-1-8b-instruct-xieshen-rhaiis.apps.ai-dev06.kni.syseng.devcluster.openshift.com\n",
      "Checking service health endpoint...\n",
      "✓ Service ready!\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test the Service:",
   "id": "c5c0f510955a47f4"
  },
  {
   "cell_type": "code",
   "id": "f4c9b082",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T19:25:05.882015Z",
     "start_time": "2026-02-04T19:24:56.140139Z"
    }
   },
   "source": [
    "result = generate_text(\n",
    "    url=endpoint[0],\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    prompt=\"Write a complete function that computes fibonacci numbers in Rust\"\n",
    ")\n",
    "print(result if result else \"Failed to generate text\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Fibonacci Function in Rust**\n",
      "================================\n",
      "\n",
      "Here is a simple function that computes Fibonacci numbers in Rust. This function uses recursion, which is a common approach for calculating Fibonacci numbers. However, please note that recursion can lead to a stack overflow for large inputs.\n",
      "\n",
      "```rust\n",
      "/// Calculate the nth Fibonacci number using recursion.\n",
      "fn fibonacci_recursive(n: u32) -> u32 {\n",
      "    match n {\n",
      "        0 => 0,\n",
      "        1 => 1,\n",
      "        _ => fibonacci_recursive(n - 1) + fibonacci_recursive(n - 2),\n",
      "    }\n",
      "}\n",
      "\n",
      "/// Calculate the nth Fibonacci number using iteration.\n",
      "fn fibonacci_iterative(n: u32) -> u32 {\n",
      "    if n <= 1 {\n",
      "        return n;\n",
      "    }\n",
      "\n",
      "    let mut a = 0;\n",
      "    let mut b = 1;\n",
      "    for _ in 2..=n {\n",
      "        let temp = a + b;\n",
      "        a = b;\n",
      "        b = temp;\n",
      "    }\n",
      "    b\n",
      "}\n",
      "\n",
      "fn main() {\n",
      "    let n = 10; // Change this to the desired Fibonacci number\n",
      "    println!(\"Fibonacci number at index {} (recursive): {}\", n, fibonacci_recursive(n));\n",
      "    println!(\"Fibonacci number at index {} (iterative): {}\", n, fibonacci_iterative(n));\n",
      "}\n",
      "```\n",
      "\n",
      "**Performance Considerations**\n",
      "-----------------------------\n",
      "\n",
      "While the recursive function is simple and easy to understand, it is not the most efficient way to calculate Fibonacci numbers, especially for large inputs. This is because each recursive call creates a new stack frame, leading to a high time complexity of O(2^n).\n",
      "\n",
      "The iterative function, on the other hand, has a time complexity of O(n) and is generally faster and more memory-efficient.\n",
      "\n",
      "**Example Use Cases**\n",
      "---------------------\n",
      "\n",
      "You can use the `fibonacci_recursive` or `fibonacci_iterative` function to calculate any Fibonacci number. Simply pass the desired index `n` as an argument.\n",
      "\n",
      "```rust\n",
      "let n = 10; // Calculate the 10th Fibonacci number\n",
      "println!(\"Fibonacci number at index {}: {}\", n, fibonacci_recursive(n));\n",
      "```\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "d0fed5f0",
   "metadata": {},
   "source": [
    "### Example 2: Customizing Model Parameters\n",
    "\n",
    "This example demonstrates how custom parameters affect model behavior. We'll deploy with specific constraints and test them:\n",
    "\n",
    "**Key Parameters:**\n",
    "* `--tensor-parallel-size=1`: Uses 1 GPU in parallel\n",
    "* `--max-model-len=2048`: Limits model context length"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. Update the Deployment custom resource (CR) with model parameters.",
   "id": "50067f5bfc2d104b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!oc apply -f ./2_custom_params/deployment.yaml -n rhaiis-namespace",
   "id": "7f1867ae3c0264d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Now let's test the deployed model:",
   "id": "1c23be3723d35b85"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check if service is ready:",
   "id": "ff007f7b30502a8c"
  },
  {
   "cell_type": "code",
   "id": "1e596290",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T03:29:59.538158Z",
     "start_time": "2026-02-05T03:29:58.869852Z"
    }
   },
   "source": [
    "endpoint = !oc get route llama-3-1-8b-instruct -n rhaiis-namespace -o jsonpath='{.spec.host}'\n",
    "\n",
    "# URL is a list, access the first element\n",
    "print(endpoint[0])\n",
    "\n",
    "check_service_ready(url=endpoint[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama-3-1-8b-instruct-xieshen-rhaiis.apps.ai-dev06.kni.syseng.devcluster.openshift.com\n",
      "Checking service health endpoint...\n",
      "✓ Service ready!\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "43c8335c",
   "metadata": {},
   "source": "Test with custom parameters:"
  },
  {
   "cell_type": "code",
   "id": "bc931c2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T03:30:17.214791Z",
     "start_time": "2026-02-05T03:30:05.278930Z"
    }
   },
   "source": [
    "result = generate_text(\n",
    "    url=endpoint[0],\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    prompt=\"Write me a function that computes fibonacci in Javascript\"\n",
    ")\n",
    "print(result if result else \"Failed to generate text\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Fibonacci Function in JavaScript**\n",
      "=====================================\n",
      "\n",
      "Here is a simple function that calculates the Fibonacci sequence in JavaScript. The Fibonacci sequence is a series of numbers where a number is the sum of the two preceding ones, usually starting with 0 and 1.\n",
      "\n",
      "```javascript\n",
      "/**\n",
      " * Calculates the nth Fibonacci number.\n",
      " *\n",
      " * @param {number} n - The position of the Fibonacci number to calculate.\n",
      " * @returns {number} The nth Fibonacci number.\n",
      " */\n",
      "function fibonacci(n) {\n",
      "  if (n <= 0) {\n",
      "    throw new Error(\"n must be a positive integer\");\n",
      "  }\n",
      "\n",
      "  if (n === 1) {\n",
      "    return 0;\n",
      "  }\n",
      "\n",
      "  if (n === 2) {\n",
      "    return 1;\n",
      "  }\n",
      "\n",
      "  let a = 0;\n",
      "  let b = 1;\n",
      "  let result = 0;\n",
      "\n",
      "  for (let i = 3; i <= n; i++) {\n",
      "    result = a + b;\n",
      "    a = b;\n",
      "    b = result;\n",
      "  }\n",
      "\n",
      "  return result;\n",
      "}\n",
      "```\n",
      "\n",
      "**Example Use Cases**\n",
      "---------------------\n",
      "\n",
      "```javascript\n",
      "console.log(fibonacci(1)); // 0\n",
      "console.log(fibonacci(2)); // 1\n",
      "console.log(fibonacci(3)); // 1\n",
      "console.log(fibonacci(4)); // 2\n",
      "console.log(fibonacci(5)); // 3\n",
      "console.log(fibonacci(6)); // 5\n",
      "console.log(fibonacci(7)); // 8\n",
      "console.log(fibonacci(8)); // 13\n",
      "console.log(fibonacci(9)); // 21\n",
      "```\n",
      "\n",
      "This function uses an iterative approach to calculate the Fibonacci sequence, which is more efficient than a recursive approach for large values of n.\n",
      "\n",
      "**Memoization for Optimization**\n",
      "------------------------------\n",
      "\n",
      "If you need to calculate multiple Fibonacci numbers, you can use memoization to store previously calculated values and avoid redundant calculations.\n",
      "\n",
      "```javascript\n",
      "const memo = { 0: 0, 1: 1 };\n",
      "\n",
      "function fibonacci(n) {\n",
      "  if (!memo[n]) {\n",
      "    for (let i = 2; i <= n; i++) {\n",
      "      memo[i] = memo[i - 1] + memo[i - 2];\n",
      "    }\n",
      "  }\n",
      "\n",
      "  return memo[n];\n",
      "}\n",
      "```\n",
      "\n",
      "This optimized version uses a memoization object to store previously calculated Fibonacci numbers, which can significantly improve performance for large values of n.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "6d0f0713",
   "metadata": {},
   "source": [
    "### Example 3: Deployment from Local Model\n",
    "\n",
    "This example shows how to deploy Qwen2.5-0.5B from the locally downloaded model:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. Create a PersistentVolumeClaim (PVC) custom resource (CR) and apply it in the cluster. You use the PVC as the location where you store the models that you download.",
   "id": "87dc494320cb2271"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!oc apply -f ./3_local_model/pvc.yaml -n rhaiis-namespace",
   "id": "9de2ce6bba2d4e84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2. Download Model to PVC\n",
    "\n",
    "We'll download Qwen2.5-0.5B, a lightweight LLM, for use in Example 3."
   ],
   "id": "49da4e41e7809ea9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!oc apply -f ./3_local_model/download-job.yaml -n rhaiis-namespace",
   "id": "9ae4e53633e34bae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Create a Deployment custom resource (CR) that uses the downloaded model from the PVC and deploys the Red Hat AI Inference Server container.",
   "id": "32712f8b2a8ff823"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!oc apply -f ./3_local_model/deployment.yaml -n rhaiis-namespace",
   "id": "c16e809ad278b895"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4. Create a Service CR for the model inference. For example:",
   "id": "a5f7f97c5f8729e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!oc apply -f ./3_local_model/service.yaml -n rhaiis-namespace",
   "id": "4ecf34bdfe19f65"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 5. Create a Route CR to enable public access to the model. For example:",
   "id": "188faa6ee1f91520"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!oc apply -f ./3_local_model/route.yaml",
   "id": "1ef1046d1e6e78a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 6. Now let's test the deployed model:",
   "id": "e4b00fd13adc2310"
  },
  {
   "cell_type": "code",
   "id": "9394bcee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T04:12:29.511468Z",
     "start_time": "2026-02-05T04:12:28.739245Z"
    }
   },
   "source": [
    "endpoint = !oc get route qwen2-5-0-5b -n rhaiis-namespace -o jsonpath='{.spec.host}'\n",
    "\n",
    "# URL is a list, access the first element\n",
    "print(endpoint[0])\n",
    "\n",
    "check_service_ready(url=endpoint[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen2-5-0-5b-xieshen-rhaiis.apps.ai-dev06.kni.syseng.devcluster.openshift.com\n",
      "Checking service health endpoint...\n",
      "✓ Service ready!\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "b3808a1b",
   "metadata": {},
   "source": [
    "Test the local model deployment:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8228a12a",
   "metadata": {
    "lines_to_next_cell": 2,
    "ExecuteTime": {
     "end_time": "2026-02-05T04:15:14.424908Z",
     "start_time": "2026-02-05T04:15:12.854944Z"
    }
   },
   "source": [
    "result = generate_text(\n",
    "    url=endpoint[0],\n",
    "    model=\"Qwen/Qwen2.5-0.5B\",\n",
    "    prompt=\"Tell me a story about a cat\")\n",
    "print(result if result else \"Failed to generate text\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a cat named Spotty. She was a very smart and funny animal who loved to play in the sun and chase after a ball across the grass. One day, Spotty’s owner, Mr. Baker, decided to take her for a walk in the park. As they walked, Spotty noticed a group of rabbits playing by the river. She decided to join them and soon found out that they were having a great time together. They played a game of fetch and Spotty even got to catch the rabbits’ ball in the process. After the walk, Spotty and Mr. Baker invited the rabbits to a picnic. They sat on a bench and enjoyed a delicious meal of carrots and apples. They even talked about their day and how they had a great time together. That was the moment when Spotty realized just how much they had in common and that they would always be friends.\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}