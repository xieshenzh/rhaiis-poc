<!--
SPDX-FileCopyrightText: Copyright (c) 2024-2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
SPDX-License-Identifier: Apache-2.0

This file has been modified from the original at
https://github.com/NVIDIA-AI-Blueprints/bring-llms-to-nim
Changes: Adapted for Red Hat AI Inference Server (RHAIIS) on OpenShift.
-->

<h2><img align="center" src="https://github.com/user-attachments/assets/cbe0d62f-c856-4e0b-b3ee-6184b7c4d96f">NVIDIA AI Blueprint: Bring Your LLM to NIM</h2>

### [Notebook 3: Deploy GGUF Checkpoints](https://github.com/NVIDIA-AI-Blueprints/bring-llms-to-nim/blob/main/deploy/3_GGUF_Checkpoints.ipynb)
**Best for:** Memory-efficient deployment with quantized models
- Deploy pre-quantized GGUF models (4-bit, 8-bit, etc.)
- Reduce GPU memory requirements significantly
- Handle external configuration requirements