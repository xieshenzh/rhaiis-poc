apiVersion: batch/v1
kind: Job
metadata:
  name: llama-3-2-3b-instruct-q8
  namespace: rhaiis-namespace
spec:
  backoffLimit: 3
  template:
    metadata:
      labels:
        app: llama-3-2-3b-instruct-q8
    spec:
      restartPolicy: OnFailure
      imagePullSecrets:
        - name: docker-secret
      volumes:
        - name: model-volume
          persistentVolumeClaim:
            claimName: llama-3-2-3b-instruct-q8
      containers:
        - name: download-model
          image: 'registry.redhat.io/rhaiis/vllm-cuda-rhel9@sha256:a6645a8e8d7928dce59542c362caf11eca94bb1b427390e78f0f8a87912041cd'
          imagePullPolicy: IfNotPresent
          envFrom:
            - secretRef:
                name: hf-secret
          env:
            - name: HF_HOME
              value: /opt/app-root/src/.cache
            - name: HF_HUB_OFFLINE
              value: "0"
            - name: MODEL_NAME
              value: "meta-llama/Llama-3.2-3B-Instruct-Q8"
          command:
            - /bin/bash
            - -c
            - |
              echo "Starting model download from HuggingFace..."
              echo "Model: $MODEL_NAME"
              echo "Download location: $HF_HOME"

              # Install huggingface-cli if not available
              pip install -q huggingface_hub

              # Download config.json from the original Llama-3.2-3B-Instruct repository
               huggingface-cli download meta-llama/Llama-3.2-3B-Instruct \
                config.json \
                --local-dir "$HF_HOME/hub/models--${MODEL_NAME//\//_}" \
                --local-dir-use-symlinks False
              
              # Also download tokenizer files that may be needed
              huggingface-cli download meta-llama/Llama-3.2-3B-Instruct \
                tokenizer.json \
                tokenizer_config.json \
                --local-dir "$HF_HOME/hub/models--${MODEL_NAME//\//_}" \
                --local-dir-use-symlinks False
              
              # Download specific GGUF model file
              huggingface-cli download bartowski/Llama-3.2-3B-Instruct-GGUF \
                Llama-3.2-3B-Instruct-Q8_0.gguf \
                --local-dir "$HF_HOME/hub/models--${MODEL_NAME//\//_}" \
                --local-dir-use-symlinks False
    
              echo "Model download completed successfully!"
              ls -lh "$HF_HOME/hub/"
          volumeMounts:
            - name: model-volume
              mountPath: /opt/app-root/src/.cache
          resources:
            limits:
              cpu: '4'
              memory: 8Gi
              nvidia.com/gpu: '1'
            requests:
              cpu: '1'
              memory: 2Gi
              nvidia.com/gpu: '1'
      tolerations:
        - effect: NoSchedule
          key: g6e-gpu
          operator: Exists
