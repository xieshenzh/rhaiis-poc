{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9779ce5b",
   "metadata": {},
   "source": "<!--\nSPDX-FileCopyrightText: Copyright (c) 2024-2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\nSPDX-License-Identifier: Apache-2.0\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n> **Note:** This file has been modified from the original\n> [NVIDIA AI Blueprint: Bring Your LLM to NIM](https://github.com/NVIDIA-AI-Blueprints/bring-llms-to-nim).\n> Changes: Adapted for Red Hat AI Inference Server (RHAIIS) on OpenShift,\n> replacing NVIDIA NIM Docker deployment with OpenShift Kubernetes resources.\n\n# Deploy GGUF Checkpoints with NIM\n\nThis notebook shows you how to deploy memory-efficient quantized models using GGUF format with NVIDIA NIM. Perfect for running large models on consumer GPUs or maximizing the number of models per server.\n\n## What You'll Build\n\nBy the end of this notebook, you'll be able to:\n- Deploy quantized models that use 50-75% less memory\n- Run large models on consumer GPUs (8GB-16GB VRAM)\n- Choose the right quantization level for your needs\n- Handle GGUF's special configuration requirements\n\n## When to Use This Approach\n\n**Choose this notebook if you:**\n- Have limited GPU memory (8GB-16GB VRAM)\n- Want to run larger models on smaller GPUs\n- Need to deploy multiple models on one GPU\n- Can accept slight quality trade-offs for efficiency\n\n**Consider other notebooks if you:**\n- Have plenty of GPU memory (See Notebook 1: HuggingFace)\n- Need maximum quality/performance (See Notebook 2: TensorRT-LLM)\n\n## Understanding GGUF Quantization\n\nQuantization reduces model size by using fewer bits for weights:\n\n| Format | Model Size | Quality | Use Case |\n|--------|------------|---------|----------|\n| Full Precision | 100% (baseline) | Perfect | Research, fine-tuning |\n| Q8_0 | ~33% | Near-perfect | Quality-focused deployment |\n| Q5_K_M | ~22% | Excellent | Balanced deployment |\n| Q4_K_M | ~18% | Very Good | **Recommended** - best balance |\n| Q3_K_M | ~14% | Good | Memory-constrained |\n\n**Example**: Llama-3.2-3B\n- Full model: ~13GB -> Won't fit on RTX 3060\n- Q4_K_M: ~2.1GB -> Runs comfortably on 8GB GPUs\n\n## The GGUF Challenge\n\nGGUF files don't include configuration metadata, so we need to:\n1. Download the GGUF model file\n2. Get the config.json from the original model\n3. Organize them correctly for NIM\n\nDon't worry - we'll walk through this step-by-step!\n\n## What's Covered\n\nThis tutorial includes:\n* **Setup**: Understanding GGUF requirements\n* **Example 1**: Deploying pre-downloaded GGUF models locally\n* **Example 2**: Comparing different quantization levels\n* **Example 3**: Custom deployment configurations\n* **Bonus**: Quick reference for all quantization options\n\n## Prerequisites\n\n### Hardware Requirements\n\nGGUF deployment is more resource-friendly than full-precision models:\n\n- **GPU**: NVIDIA GPU with at least 8GB VRAM (for Llama-3.2-3B with Q4_K_M quantization)\n  - Recommended: RTX 4070, RTX 3080, or higher\n  - Supported: RTX 3060 12GB, RTX 4060 Ti 16GB for smaller models\n- **System Memory**: At least 16GB RAM recommended\n- **Storage**: 5-15GB free space depending on quantization level\n\n**Model Size Estimates (Llama-3.2-3B):**\n- Q4_K_M: ~2.1GB (recommended balance of quality/size)\n- Q5_K_M: ~2.6GB (higher quality)\n- Q8_0: ~3.2GB (highest quality quantized)\n\nFor detailed hardware specifications, refer to the [NIM LLM Documentation](https://docs.nvidia.com/nim/large-language-models/latest/getting-started.html).\n\n### System Setup\n\nFirst, let's verify your GPU setup and install necessary dependencies:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373b5146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6c535e",
   "metadata": {},
   "source": [
    "### Install Required Software\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f897d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python dependencies\n",
    "!python -m ensurepip --upgrade\n",
    "%pip install docker requests huggingface-hub && echo \"✓ Python dependencies installed successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fffce74",
   "metadata": {},
   "source": [
    "### Get API Keys\n",
    "\n",
    "#### NVIDIA NGC API Key\n",
    "\n",
    "The NVIDIA NGC API Key is mandatory for accessing NVIDIA container registry and pulling secure container images.\n",
    "Refer to [Generating NGC API Keys](https://docs.nvidia.com/ngc/gpu-cloud/ngc-user-guide/index.html#generating-api-key) for more information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3736d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Replace with your actual NGC API key\n",
    "if not os.environ.get(\"NGC_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    ngc_api_key = getpass.getpass(\"Enter your NGC API Key: \")\n",
    "    assert ngc_api_key.startswith(\"nvapi-\"), \"Not a valid key\"\n",
    "    os.environ[\"NGC_API_KEY\"] = ngc_api_key\n",
    "    print(\"✓ NGC API Key set successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59da9db",
   "metadata": {},
   "source": [
    "### Docker Login\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39196af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to NGC registry\n",
    "!echo \"$NGC_API_KEY\" | docker login nvcr.io -u '$oauthtoken' --password-stdin && echo \"✓ Docker login successful\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e17588",
   "metadata": {},
   "source": [
    "#### Hugging Face Token\n",
    "\n",
    "You'll also need a [Huggingface Token](https://huggingface.co/settings/tokens) to download models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a00f21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"HF_TOKEN\", \"\").startswith(\"hf_\"):\n",
    "    hf_token = getpass.getpass(\"Enter your Huggingface Token: \")\n",
    "    assert hf_token.startswith(\"hf_\"), \"Not a valid key\"\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token\n",
    "    print(\"✓ Hugging Face token set successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675db73a",
   "metadata": {},
   "source": [
    "### Setup NIM Container\n",
    "\n",
    "Choose your NIM container image and pull it:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e95e57a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Set the NIM image\n",
    "os.environ['NIM_IMAGE'] = \"nvcr.io/nim/nvidia/llm-nim:latest\"\n",
    "print(f\"Using NIM image: {os.environ['NIM_IMAGE']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac4c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the NIM container image\n",
    "!docker pull $NIM_IMAGE && echo \"✓ NIM container image pulled successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0139da0",
   "metadata": {},
   "source": [
    "### Setup Common Variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cc3c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set base directory for all files - you can modify this path as needed\n",
    "# Examples: \".\", \"~\", \"/tmp\", \"/scratch\", etc.\n",
    "base_work_dir = \"/ephemeral\"\n",
    "os.environ[\"BASE_WORK_DIR\"] = base_work_dir\n",
    "\n",
    "os.environ[\"CONTAINER_NAME\"] = \"GGUF-NIM\"\n",
    "os.environ[\"LOCAL_NIM_CACHE\"] = os.path.join(base_work_dir, \".cache/nim\")\n",
    "os.environ[\"GGUF_WORK_DIR\"] = os.path.join(base_work_dir, \"gguf_models\")\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(os.environ[\"LOCAL_NIM_CACHE\"], exist_ok=True)\n",
    "os.makedirs(os.environ[\"GGUF_WORK_DIR\"], exist_ok=True)\n",
    "\n",
    "print(\"✓ Directories created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f620ce4",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "\n",
    "Below are some utility functions we'll use in this notebook. These are for simplifying the process of deploying and monitoring NIMs in a notebook environment, and aren't required in general.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc177a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import docker\n",
    "import os\n",
    "\n",
    "def check_service_ready_from_logs(container_name, print_logs=False, timeout=600):\n",
    "    \"\"\"\n",
    "    Check if NIM service is ready by monitoring Docker logs for 'Application startup complete' message.\n",
    "\n",
    "    Args:\n",
    "        container_name (str): Name of the Docker container\n",
    "        print_logs (bool): Whether to print logs while monitoring (default: False)\n",
    "        timeout (int): Maximum time to wait in seconds (default: 600)\n",
    "\n",
    "    Returns:\n",
    "        bool: True if service is ready, False if timeout reached\n",
    "    \"\"\"\n",
    "    print(\"Waiting for NIM service to start...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        client = docker.from_env()\n",
    "        container = client.containers.get(container_name)\n",
    "\n",
    "        # Stream logs in real-time using the blocking generator\n",
    "        log_buffer = \"\"\n",
    "        for log_chunk in container.logs(stdout=True, stderr=True, follow=True, stream=True):\n",
    "            # Check timeout\n",
    "            if time.time() - start_time > timeout:\n",
    "                print(f\"❌ Timeout reached ({timeout}s). Service may not have started properly.\")\n",
    "                return False\n",
    "\n",
    "            # Decode chunk and add to buffer\n",
    "            chunk = log_chunk.decode('utf-8', errors='ignore')\n",
    "            log_buffer += chunk\n",
    "\n",
    "            # Process complete lines\n",
    "            while '\\n' in log_buffer:\n",
    "                line, log_buffer = log_buffer.split('\\n', 1)\n",
    "                line = line.strip()\n",
    "\n",
    "                if print_logs and line:\n",
    "                    print(f\"[LOG] {line}\")\n",
    "\n",
    "                # Check for startup complete message\n",
    "                if \"Application startup complete\" in line:\n",
    "                    print(\"✓ Application startup complete! Service is ready.\")\n",
    "                    return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        return False\n",
    "\n",
    "    print(f\"❌ Timeout reached ({timeout}s). Service may not have started properly.\")\n",
    "    return False\n",
    "\n",
    "def check_service_ready():\n",
    "    \"\"\"Fallback health check using HTTP endpoint\"\"\"\n",
    "    url = 'http://localhost:8000/v1/health/ready'\n",
    "    print(\"Checking service health endpoint...\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(url, headers={'accept': 'application/json'})\n",
    "            if response.status_code == 200 and response.json().get(\"message\") == \"Service is ready.\":\n",
    "                print(\"✓ Service ready!\")\n",
    "                break\n",
    "        except requests.ConnectionError:\n",
    "            pass\n",
    "        print(\"⏳ Still starting...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "def generate_text(model, prompt, max_tokens=1000, temperature=0.7):\n",
    "    \"\"\"Generate text using the NIM service\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"http://localhost:8000/v1/chat/completions\",\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": temperature\n",
    "            },\n",
    "            timeout=60\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()['choices'][0]['message']['content']\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making request: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"✓ Utility functions loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c3c7e2",
   "metadata": {},
   "source": [
    "## GGUF Deployment Examples\n",
    "\n",
    "Let's explore how to deploy GGUF models locally using NIM.\n",
    "\n",
    "## Example 1: Pre-download and Local GGUF Deployment\n",
    "\n",
    "This example shows how to pre-download GGUF models and deploy them locally. This approach provides reliable offline usage and faster startup times since models are already available locally.\n",
    "\n",
    "### Download External Config File\n",
    "\n",
    "GGUF repositories don't include the config.json file needed by NIM. We need to download it from the original Llama-3.2-3B-Instruct repository:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24efc189",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create a temporary directory for the config file\n",
    "config_temp_dir = os.path.join(base_work_dir, \"gguf_config_temp\")\n",
    "os.makedirs(config_temp_dir, exist_ok=True)\n",
    "os.environ[\"CONFIG_TEMP_DIR\"] = config_temp_dir\n",
    "\n",
    "# Download config.json from the original Llama-3.2-3B-Instruct repository\n",
    "print(\"Downloading config.json from original model repository...\")\n",
    "!wget --header=\"Authorization: Bearer $HF_TOKEN\" -O \"$CONFIG_TEMP_DIR/config.json\" https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json && echo \"✓ Config file downloaded successfully\"\n",
    "\n",
    "# Also download tokenizer files that may be needed\n",
    "!wget --header=\"Authorization: Bearer $HF_TOKEN\" -O \"$CONFIG_TEMP_DIR/tokenizer.json\" https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/resolve/main/tokenizer.json 2>/dev/null || echo \"tokenizer.json not found - continuing\"\n",
    "!wget --header=\"Authorization: Bearer $HF_TOKEN\" -O \"$CONFIG_TEMP_DIR/tokenizer_config.json\" https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/resolve/main/tokenizer_config.json 2>/dev/null || echo \"tokenizer_config.json not found - continuing\"\n",
    "\n",
    "print(\"✓ Finished downloading configuration files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2ddea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the config file was downloaded\n",
    "!ls -Rlh \"$CONFIG_TEMP_DIR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a18e2f",
   "metadata": {},
   "source": [
    "### Download GGUF Model Locally\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a46dee",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "q8_model_path = os.path.join(os.environ[\"GGUF_WORK_DIR\"], \"Llama-3.2-3B-Instruct-Q8_0\")\n",
    "os.makedirs(q8_model_path, exist_ok=True)\n",
    "os.environ[\"Q8_MODEL_PATH\"] = q8_model_path\n",
    "\n",
    "# Download specific GGUF model files to their respective directories\n",
    "print(\"Downloading GGUF model files locally...\")\n",
    "print(\"This may take several minutes depending on your internet connection...\")\n",
    "\n",
    "# Download the Q8_0 quantization\n",
    "!wget --header=\"Authorization: Bearer $HF_TOKEN\" -O \"$Q8_MODEL_PATH/Llama-3.2-3B-Instruct-Q8_0.gguf\" \\\n",
    "  https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q8_0.gguf && \\\n",
    "  echo \"✓ Q8_0 quantization downloaded successfully\"\n",
    "\n",
    "print(\"✓ GGUF model files downloaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25271ca9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "q4_model_path = os.path.join(os.environ[\"GGUF_WORK_DIR\"], \"Llama-3.2-3B-Instruct-Q4_K_M\")\n",
    "\n",
    "os.makedirs(q4_model_path, exist_ok=True)\n",
    "\n",
    "os.environ[\"Q4_MODEL_PATH\"] = q4_model_path\n",
    "\n",
    "# Download specific GGUF model files to their respective directories\n",
    "print(\"Downloading GGUF model files locally...\")\n",
    "print(\"This may take several minutes depending on your internet connection...\")\n",
    "\n",
    "# Download the Q4_K_M quantization\n",
    "!wget --header=\"Authorization: Bearer $HF_TOKEN\" -O \"$Q4_MODEL_PATH/Llama-3.2-3B-Instruct-Q4_K_M.gguf\" \\\n",
    "  https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf && \\\n",
    "  echo \"✓ Q4_K_M quantization downloaded successfully\"\n",
    "\n",
    "print(\"✓ GGUF model files downloaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dafd751",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Verify the download and check available quantization files\n",
    "!ls -lha $Q4_MODEL_PATH\n",
    "!ls -lha $Q8_MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3306e885",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Copy configuration files to both quantization directories\n",
    "!cp $CONFIG_TEMP_DIR/config.json $Q4_MODEL_PATH/ && echo \"✓ Config file copied to Q4_K_M directory\"\n",
    "!cp $CONFIG_TEMP_DIR/tokenizer*.json $Q4_MODEL_PATH/ 2>/dev/null || echo \"Some tokenizer files not found - continuing\"\n",
    "\n",
    "!cp $CONFIG_TEMP_DIR/config.json $Q8_MODEL_PATH/ && echo \"✓ Config file copied to Q8_0 directory\"\n",
    "!cp $CONFIG_TEMP_DIR/tokenizer*.json $Q8_MODEL_PATH/ 2>/dev/null || echo \"Some tokenizer files not found - continuing\"\n",
    "\n",
    "print(\"✓ Configuration files copied to all quantization directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9634c4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the complete model setup for both quantizations\n",
    "!echo \"Q4_K_M model directory:\"\n",
    "!ls -Rlh $Q4_MODEL_PATH\n",
    "!echo\n",
    "!echo \"Q8_0 model directory:\"\n",
    "!ls -Rlh $Q8_MODEL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f111d7e7",
   "metadata": {},
   "source": [
    "### Deploy Local GGUF Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9504bb2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Deploy Q8_0 quantization\n",
    "print(\"Deploying Q8_0 quantization\")\n",
    "\n",
    "!docker run -it --rm \\\n",
    "  --name=$CONTAINER_NAME \\\n",
    "  --runtime=nvidia \\\n",
    "  --gpus all \\\n",
    "  --shm-size=16GB \\\n",
    "  -e NIM_MODEL_NAME=\"/opt/models/q8_model\" \\\n",
    "  -e NIM_SERVED_MODEL_NAME=\"meta-llama/Llama-3.2-3B-Instruct-Q8\" \\\n",
    "  -v \"$Q8_MODEL_PATH:/opt/models/q8_model\" \\\n",
    "  -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    "  -u $(id -u) \\\n",
    "  -p 8000:8000 \\\n",
    "  -d \\\n",
    "  $NIM_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d30ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the log-based check (set print_logs=True to see detailed logs)\n",
    "check_service_ready_from_logs(os.environ[\"CONTAINER_NAME\"], print_logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627dd3c5",
   "metadata": {},
   "source": [
    "### Test Local GGUF Deployment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d201667f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Test Q8_0 quantization\n",
    "result = generate_text(\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct-Q8\",\n",
    "    prompt=\"Write a brief story about a robot learning to paint\",\n",
    ")\n",
    "print(\"Q8_0 Quantization Result:\")\n",
    "print(\"=\" * 50)\n",
    "print(result if result else \"Failed to generate text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e889771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the current deployment\n",
    "!docker stop $CONTAINER_NAME 2>/dev/null || echo \"Container already stopped\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd900cdf",
   "metadata": {},
   "source": [
    "## Available Quantization Levels\n",
    "\n",
    "The bartowski/Llama-3.2-3B-Instruct-GGUF repository includes multiple quantization levels. Each quantization requires its own directory with the GGUF file and configuration files.\n",
    "\n",
    "### Directory Structure for Each Quantization:\n",
    "\n",
    "Each quantization needs to be organized as follows:\n",
    "```\n",
    "quantization_directory/\n",
    "├── config.json                    # From original model repo\n",
    "├── tokenizer.json                 # From original model repo\n",
    "├── tokenizer_config.json          # From original model repo\n",
    "└── model_name-QUANTIZATION.gguf   # The quantized model file\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eac36e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated deploying GGUF checkpoints with NIM for memory-efficient model deployment.\n",
    "\n",
    "**Key Accomplishments:**\n",
    "- Set up complete GGUF deployment environment with NGC API keys and Docker\n",
    "- Downloaded and deployed both Q4_K_M (~2.1GB) and Q8_0 (~3.2GB) quantizations of Llama-3.2-3B\n",
    "- Handled GGUF's special requirements for external config files from original model repositories\n",
    "- Achieved 50-75% memory reduction compared to full-precision models\n",
    "\n",
    "**Technical Highlights:**\n",
    "- GGUF models need external `config.json` files and separate directories per quantization\n",
    "- Universal NIM container supports GGUF format with proper configuration\n",
    "- Q4_K_M offers best balance of quality/efficiency; Q8_0 provides near-perfect quality\n",
    "\n",
    "**You Can Now:**\n",
    "- Deploy large models on consumer GPUs (8GB-16GB VRAM)\n",
    "- Set up offline deployments with faster startup times\n",
    "- Choose quantization levels based on your quality/memory trade-offs\n",
    "\n",
    "**Next Steps:** Explore other quantization levels, try different GGUF models from community repositories, and set up production deployments with monitoring.\n",
    "\n",
    "For more GGUF models and documentation, check Hugging Face community repositories.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}